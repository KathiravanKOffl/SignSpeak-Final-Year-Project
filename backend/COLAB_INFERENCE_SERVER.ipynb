{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ SignSpeak - Colab Inference Server\n",
                "\n",
                "This notebook runs your trained ASL model on Google Colab GPU and exposes it via **Cloudflare Tunnel**.\n",
                "\n",
                "## Setup Steps:\n",
                "1. Upload model files (best_model.pth, label_mapping.json)\n",
                "2. Run all cells\n",
                "3. Copy tunnel URL (looks like: `https://xxx.trycloudflare.com`)\n",
                "4. Add to Cloudflare Pages env vars\n",
                "\n",
                "**Runtime:** GPU (T4 recommended)\n",
                "\n",
                "**âœ¨ No account needed!** Cloudflare Tunnel is completely free and anonymous."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“¦ Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install fastapi uvicorn torch mediapipe numpy pydantic nest-asyncio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŒ Step 2: Install Cloudflare Tunnel (cloudflared)\n",
                "\n",
                "No registration or auth token needed!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
                "!chmod +x cloudflared-linux-amd64\n",
                "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared\n",
                "\n",
                "print(\"âœ… Cloudflare Tunnel installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“¤ Step 3: Upload Model Files\n",
                "\n",
                "Upload these files from your Downloads folder:\n",
                "- `best_model.pth` (~11 MB)\n",
                "- `label_mapping.json`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "# Create checkpoints directory\n",
                "os.makedirs('/content/checkpoints', exist_ok=True)\n",
                "\n",
                "print(\"ðŸ“¤ Upload best_model.pth and label_mapping.json:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Move to checkpoints folder\n",
                "for filename in uploaded.keys():\n",
                "    os.rename(filename, f'/content/checkpoints/{filename}')\n",
                "    print(f\"âœ… Moved {filename} to /content/checkpoints/\")\n",
                "\n",
                "# Verify\n",
                "!ls -lh /content/checkpoints/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§  Step 4: Define Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class TemporalLSTM(nn.Module):\n",
                "    \"\"\"\n",
                "    LSTM that processes temporal sequences of landmarks\n",
                "    Input: (batch, 30, 150) - 30 frames, 150 features each\n",
                "    \"\"\"\n",
                "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers, dropout):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Input projection\n",
                "        self.input_proj = nn.Sequential(\n",
                "            nn.Linear(input_dim, hidden_dim),\n",
                "            nn.LayerNorm(hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout)\n",
                "        )\n",
                "        \n",
                "        # Bidirectional LSTM\n",
                "        self.lstm = nn.LSTM(\n",
                "            input_size=hidden_dim,\n",
                "            hidden_size=hidden_dim,\n",
                "            num_layers=num_layers,\n",
                "            batch_first=True,\n",
                "            bidirectional=True,\n",
                "            dropout=dropout if num_layers > 1 else 0\n",
                "        )\n",
                "        \n",
                "        # Classification head\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(hidden_dim, num_classes)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (batch, 30, 150)\n",
                "        x = self.input_proj(x)  # (batch, 30, hidden)\n",
                "        lstm_out, (h_n, c_n) = self.lstm(x)  # lstm_out: (batch, 30, hidden*2)\n",
                "        \n",
                "        # Use last hidden states from both directions\n",
                "        forward_h = h_n[-2]  # Last layer, forward\n",
                "        backward_h = h_n[-1]  # Last layer, backward\n",
                "        combined = torch.cat([forward_h, backward_h], dim=1)  # (batch, hidden*2)\n",
                "        \n",
                "        return self.classifier(combined)\n",
                "\n",
                "print(\"âœ… Model architecture defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“¥ Step 5: Load Trained Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Load checkpoint\n",
                "checkpoint = torch.load('/content/checkpoints/best_model.pth', map_location=device)\n",
                "\n",
                "# Extract config\n",
                "config = checkpoint.get('config', {})\n",
                "num_classes = checkpoint.get('num_classes', 25)\n",
                "input_dim = checkpoint.get('input_dim', 150)\n",
                "hidden_dim = config.get('hidden_dim', 256)\n",
                "num_layers = config.get('lstm_layers', 2)\n",
                "dropout = config.get('dropout', 0.4)\n",
                "\n",
                "print(f\"Model config: {num_classes} classes, {input_dim} features\")\n",
                "\n",
                "# Create model\n",
                "model = TemporalLSTM(\n",
                "    input_dim=input_dim,\n",
                "    hidden_dim=hidden_dim,\n",
                "    num_classes=num_classes,\n",
                "    num_layers=num_layers,\n",
                "    dropout=dropout\n",
                ").to(device)\n",
                "\n",
                "# Load weights\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "print(f\"âœ… Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
                "\n",
                "# Load label mapping\n",
                "with open('/content/checkpoints/label_mapping.json', 'r') as f:\n",
                "    label_data = json.load(f)\n",
                "    id_to_label = {int(k): v for k, v in label_data['id_to_label'].items()}\n",
                "\n",
                "print(f\"âœ… Loaded mapping for {len(id_to_label)} classes\")\n",
                "print(f\"Classes: {list(id_to_label.values())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŒ Step 6: Create FastAPI Server"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from fastapi import FastAPI\n",
                "from fastapi.middleware.cors import CORSMiddleware\n",
                "from pydantic import BaseModel\n",
                "from typing import Dict, Any, List\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "app = FastAPI(title=\"SignSpeak ASL Inference\")\n",
                "\n",
                "# Enable CORS\n",
                "app.add_middleware(\n",
                "    CORSMiddleware,\n",
                "    allow_origins=[\"*\"],\n",
                "    allow_credentials=True,\n",
                "    allow_methods=[\"*\"],\n",
                "    allow_headers=[\"*\"],\n",
                ")\n",
                "\n",
                "class LandmarkRequest(BaseModel):\n",
                "    landmarks: Dict[str, Any]\n",
                "    language: str = 'asl'\n",
                "    top_k: int = 5\n",
                "\n",
                "@app.get(\"/\")\n",
                "def root():\n",
                "    return {\n",
                "        \"status\": \"ok\",\n",
                "        \"service\": \"SignSpeak ASL Inference\",\n",
                "        \"model\": \"TemporalLSTM\",\n",
                "        \"device\": str(device),\n",
                "        \"classes\": num_classes\n",
                "    }\n",
                "\n",
                "@app.get(\"/health\")\n",
                "def health():\n",
                "    return {\n",
                "        \"status\": \"healthy\",\n",
                "        \"model_loaded\": True,\n",
                "        \"gpu_available\": torch.cuda.is_available()\n",
                "    }\n",
                "\n",
                "@app.post(\"/predict\")\n",
                "def predict(request: LandmarkRequest):\n",
                "    start_time = time.time()\n",
                "    \n",
                "    try:\n",
                "        # Extract landmarks\n",
                "        landmarks = request.landmarks\n",
                "        \n",
                "        # Assuming landmarks come as flat arrays from MediaPipe\n",
                "        # We need to reshape to (30, 150) for our LSTM\n",
                "        if isinstance(landmarks, dict):\n",
                "            # Combine all landmark types\n",
                "            pose = np.array(landmarks.get('pose', []))\n",
                "            left_hand = np.array(landmarks.get('leftHand', []))\n",
                "            right_hand = np.array(landmarks.get('rightHand', []))\n",
                "            face = np.array(landmarks.get('face', []))\n",
                "            \n",
                "            # For now, use right hand (21 landmarks Ã— 2 coords = 42 features)\n",
                "            # Pad to 150 and repeat for 30 frames\n",
                "            if len(right_hand) > 0:\n",
                "                right_hand_flat = right_hand.flatten()[:150]\n",
                "                # Pad if needed\n",
                "                if len(right_hand_flat) < 150:\n",
                "                    right_hand_flat = np.pad(right_hand_flat, (0, 150 - len(right_hand_flat)))\n",
                "                # Repeat for 30 frames\n",
                "                input_tensor = np.tile(right_hand_flat, (30, 1))\n",
                "            else:\n",
                "                return {\"error\": \"No hand landmarks detected\"}\n",
                "        else:\n",
                "            return {\"error\": \"Invalid landmark format\"}\n",
                "        \n",
                "        # Convert to tensor\n",
                "        input_tensor = torch.FloatTensor(input_tensor).unsqueeze(0).to(device)  # (1, 30, 150)\n",
                "        \n",
                "        # Inference\n",
                "        with torch.no_grad():\n",
                "            output = model(input_tensor)\n",
                "            probs = torch.softmax(output, dim=1)\n",
                "            top_probs, top_indices = torch.topk(probs, min(request.top_k, num_classes))\n",
                "        \n",
                "        # Format response\n",
                "        predictions = []\n",
                "        for i in range(len(top_indices[0])):\n",
                "            idx = top_indices[0][i].item()\n",
                "            prob = top_probs[0][i].item()\n",
                "            predictions.append({\n",
                "                \"gloss\": id_to_label[idx],\n",
                "                \"confidence\": float(prob)\n",
                "            })\n",
                "        \n",
                "        processing_time = (time.time() - start_time) * 1000  # ms\n",
                "        \n",
                "        return {\n",
                "            \"predictions\": predictions,\n",
                "            \"gloss\": predictions[0][\"gloss\"],\n",
                "            \"confidence\": predictions[0][\"confidence\"],\n",
                "            \"language\": request.language,\n",
                "            \"processing_time_ms\": processing_time\n",
                "        }\n",
                "    \n",
                "    except Exception as e:\n",
                "        return {\"error\": str(e)}\n",
                "\n",
                "print(\"âœ… FastAPI server created\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸš€ Step 7: Start Server with Cloudflare Tunnel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nest_asyncio\n",
                "import uvicorn\n",
                "from threading import Thread\n",
                "import subprocess\n",
                "import time\n",
                "import re\n",
                "\n",
                "# Allow nested asyncio (needed for Colab)\n",
                "nest_asyncio.apply()\n",
                "\n",
                "# Start FastAPI server in background\n",
                "def run_server():\n",
                "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"warning\")\n",
                "\n",
                "server_thread = Thread(target=run_server, daemon=True)\n",
                "server_thread.start()\n",
                "\n",
                "print(\"â³ Starting server...\")\n",
                "time.sleep(3)\n",
                "print(\"âœ… Server running on port 8000\")\n",
                "\n",
                "# Start Cloudflare Tunnel in background\n",
                "print(\"\\nðŸŒ Starting Cloudflare Tunnel...\")\n",
                "tunnel_process = subprocess.Popen(\n",
                "    ['cloudflared', 'tunnel', '--url', 'http://localhost:8000'],\n",
                "    stdout=subprocess.PIPE,\n",
                "    stderr=subprocess.PIPE,\n",
                "    text=True\n",
                ")\n",
                "\n",
                "# Wait for tunnel URL\n",
                "tunnel_url = None\n",
                "for line in tunnel_process.stderr:\n",
                "    if 'trycloudflare.com' in line:\n",
                "        # Extract URL from line\n",
                "        match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', line)\n",
                "        if match:\n",
                "            tunnel_url = match.group(0)\n",
                "            break\n",
                "\n",
                "if tunnel_url:\n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"ðŸŽ‰ SERVER IS RUNNING!\")\n",
                "    print(\"=\"*80)\n",
                "    print(f\"\\nðŸ“¡ Cloudflare Tunnel URL: {tunnel_url}\")\n",
                "    print(f\"\\nðŸ”§ Add this to Cloudflare Pages environment variables:\")\n",
                "    print(f\"   COLAB_TUNNEL_URL = {tunnel_url}\")\n",
                "    print(\"\\nâœ… Test it:\")\n",
                "    print(f\"   !curl {tunnel_url}/health\")\n",
                "    print(\"\\nâš ï¸  Keep this notebook running - server will stop if you close it!\")\n",
                "    print(\"=\"*80)\n",
                "else:\n",
                "    print(\"âŒ Could not extract tunnel URL. Check tunnel_process output.\")\n",
                "\n",
                "# Keep running and show status\n",
                "try:\n",
                "    while True:\n",
                "        time.sleep(60)\n",
                "        print(f\"âœ… Server alive - {time.strftime('%H:%M:%S')} - URL: {tunnel_url}\")\nexcept KeyboardInterrupt:\n",
                "    print(\"\\nðŸ›‘ Shutting down...\")\n",
                "    tunnel_process.terminate()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
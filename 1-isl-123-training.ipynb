{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ ISL-123 Training - Complete & Working\n",
                "\n",
                "**Everything in one place - Auto-generates labels, filters correctly, trains with augmentation**\n",
                "\n",
                "## What this notebook does:\n",
                "1. âœ… Auto-generates `file_to_label.json` from INCLUDE dataset\n",
                "2. âœ… **Filters to ONLY the 123 target classes** (not 158!)\n",
                "3. âœ… Removes classes with <2 samples (stratification requirement)\n",
                "4. âœ… Creates stratified 80/20 train/val split\n",
                "5. âœ… Trains with 7 augmentation techniques\n",
                "6. âœ… Targets **73-78% validation accuracy**\n",
                "\n",
                "## Required Inputs:\n",
                "- `INCLUDE` dataset (source videos)\n",
                "- `isl-123-cache` (pre-extracted landmarks)\n",
                "\n",
                "## Expected Runtime:\n",
                "- Label generation: ~30 seconds\n",
                "- Training: ~6-8 minutes\n",
                "- **Total: ~7-9 minutes**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 1: Setup\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "import numpy as np\n",
                "import json\n",
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "import time\n",
                "import random\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "def set_seed(seed=42):\n",
                "    torch.manual_seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    random.seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "set_seed(42)\n",
                "print(\"\\nâœ… Setup complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 2: Configuration\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "CONFIG = {\n",
                "    # Data\n",
                "    'seq_len': 60,\n",
                "    'input_dim': 408,\n",
                "    'num_classes': 123,\n",
                "    'train_split': 0.80,\n",
                "    \n",
                "    # Model\n",
                "    'hidden_dim': 512,\n",
                "    'num_heads': 8,\n",
                "    'num_layers': 4,\n",
                "    'dropout': 0.3,\n",
                "    \n",
                "    # Training\n",
                "    'batch_size': 32,\n",
                "    'epochs': 100,\n",
                "    'learning_rate': 1e-4,\n",
                "    'weight_decay': 0.01,\n",
                "    'label_smoothing': 0.1,\n",
                "    'gradient_clip': 1.0,\n",
                "    'patience': 15,\n",
                "    'min_delta': 0.001,\n",
                "    \n",
                "    # Augmentation (moderate strength)\n",
                "    'aug_time_warp_prob': 0.3,\n",
                "    'aug_time_warp_range': (0.9, 1.1),\n",
                "    'aug_noise_prob': 0.4,\n",
                "    'aug_noise_std': 0.015,\n",
                "    'aug_rotation_prob': 0.3,\n",
                "    'aug_rotation_range': (-10, 10),\n",
                "    'aug_scaling_prob': 0.3,\n",
                "    'aug_scaling_range': (0.95, 1.05),\n",
                "    'aug_masking_prob': 0.2,\n",
                "    'aug_masking_ratio': 0.05,\n",
                "    'aug_temporal_shift_prob': 0.2,\n",
                "    'aug_mixup_prob': 0.1,\n",
                "    'aug_mixup_alpha': 0.1,\n",
                "    \n",
                "    # Paths\n",
                "    'include_dir': '/kaggle/input/include',\n",
                "    'cache_dir': '/kaggle/input/isl-123-cache/isl_cache_123',\n",
                "    'mapping_file': '/kaggle/input/isl-123-cache/label_mapping_123.json',\n",
                "    'save_dir': '/kaggle/working'\n",
                "}\n",
                "\n",
                "print(\"âœ… Config loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 3: Load Target Classes (123 classes only)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "print(\"=\"*60)\n",
                "print(\"ðŸ“‹ LOADING TARGET CLASSES\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "with open(CONFIG['mapping_file']) as f:\n",
                "    mapping = json.load(f)\n",
                "    label_to_id = mapping['label_to_id']\n",
                "    id_to_label = {int(k): v for k, v in mapping['id_to_label'].items()}\n",
                "\n",
                "# Create set of target classes (lowercase)\n",
                "target_classes = set(cls.lower() for cls in label_to_id.keys())\n",
                "\n",
                "print(f\"\\nâœ… Target: {len(target_classes)} classes\")\n",
                "print(f\"\\nFirst 10:\")\n",
                "for i, cls in enumerate(sorted(target_classes)[:10]):\n",
                "    print(f\"   {i+1}. {cls}\")\n",
                "\n",
                "print(f\"\\nâœ… Will filter INCLUDE to ONLY these {len(target_classes)} classes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 4: Auto-Generate file_to_label.json (FILTERED!)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ðŸ”§ AUTO-GENERATING file_to_label.json\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "INCLUDE_DIR = Path(CONFIG['include_dir'])\n",
                "CACHE_DIR = Path(CONFIG['cache_dir'])\n",
                "\n",
                "# Get cache files\n",
                "cache_files = {f.stem: f.name for f in CACHE_DIR.glob('*.npy')}\n",
                "print(f\"\\nðŸ“¦ Cache files: {len(cache_files)}\")\n",
                "\n",
                "# Scan INCLUDE (FILTERED to target classes only!)\n",
                "print(f\"\\nðŸ” Scanning INCLUDE (filtering to 123 target classes)...\")\n",
                "video_to_class = {}\n",
                "\n",
                "for root, dirs, files in os.walk(INCLUDE_DIR):\n",
                "    for file in files:\n",
                "        if file.lower().endswith(('.mov', '.mp4')):\n",
                "            relative = Path(root).relative_to(INCLUDE_DIR)\n",
                "            parts = relative.parts\n",
                "            \n",
                "            if len(parts) >= 3:\n",
                "                class_folder = parts[2]\n",
                "                \n",
                "                if '. ' in class_folder:\n",
                "                    class_name = class_folder.split('. ', 1)[1].strip().lower()\n",
                "                else:\n",
                "                    class_name = class_folder.strip().lower()\n",
                "                \n",
                "                # âœ… KEY FIX: Only include if in target 123 classes!\n",
                "                if class_name in target_classes:\n",
                "                    video_stem = Path(file).stem\n",
                "                    video_to_class[video_stem] = class_name\n",
                "\n",
                "print(f\"âœ… Found {len(video_to_class)} videos in target 123 classes\")\n",
                "\n",
                "# Match cache â†’ classes\n",
                "print(f\"\\nðŸ”— Matching cache files...\")\n",
                "file_to_label_all = {}\n",
                "\n",
                "for cache_stem, cache_name in tqdm(cache_files.items(), desc=\"Matching\"):\n",
                "    if cache_stem in video_to_class:\n",
                "        file_to_label_all[cache_name] = video_to_class[cache_stem]\n",
                "    else:\n",
                "        # Partial match\n",
                "        for video_stem, class_name in video_to_class.items():\n",
                "            if video_stem in cache_stem or cache_stem in video_stem:\n",
                "                file_to_label_all[cache_name] = class_name\n",
                "                break\n",
                "\n",
                "print(f\"\\nðŸ“Š Matched: {len(file_to_label_all)}/{len(cache_files)}\")\n",
                "\n",
                "# âœ… Filter out classes with <2 samples (stratification requires â‰¥2)\n",
                "class_counts = Counter(file_to_label_all.values())\n",
                "classes_to_keep = {cls for cls, count in class_counts.items() if count >= 2}\n",
                "\n",
                "file_to_label = {\n",
                "    fname: label \n",
                "    for fname, label in file_to_label_all.items() \n",
                "    if label in classes_to_keep\n",
                "}\n",
                "\n",
                "removed = len(file_to_label_all) - len(file_to_label)\n",
                "if removed > 0:\n",
                "    print(f\"\\nðŸ§¹ Filtered out {removed} samples (classes with <2 samples)\")\n",
                "\n",
                "# Final stats\n",
                "class_counts = Counter(file_to_label.values())\n",
                "\n",
                "print(f\"\\nðŸ“ˆ Final Results:\")\n",
                "print(f\"   Samples: {len(file_to_label)}\")\n",
                "print(f\"   Classes: {len(class_counts)}/{len(target_classes)}\")\n",
                "print(f\"   Min/class: {min(class_counts.values())}\")\n",
                "print(f\"   Max/class: {max(class_counts.values())}\")\n",
                "print(f\"   Avg/class: {len(file_to_label)/len(class_counts):.1f}\")\n",
                "\n",
                "# Validation\n",
                "if min(class_counts.values()) < 2:\n",
                "    raise ValueError(\"âŒ Some classes have <2 samples - can't stratify!\")\n",
                "\n",
                "if len(class_counts) < 100:\n",
                "    print(f\"\\nâš ï¸  Only {len(class_counts)} classes (expected ~123)\")\n",
                "\n",
                "# Save\n",
                "with open(Path(CONFIG['save_dir']) / 'file_to_label.json', 'w') as f:\n",
                "    json.dump(file_to_label, f, indent=2, sort_keys=True)\n",
                "\n",
                "print(f\"\\nðŸ’¾ Saved: file_to_label.json\")\n",
                "print(f\"âœ… Filtering complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 5: Stratified Split\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ‚ï¸  STRATIFIED SPLIT\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "filenames = list(file_to_label.keys())\n",
                "labels = [file_to_label[f] for f in filenames]\n",
                "\n",
                "train_files, val_files, train_labels, val_labels = train_test_split(\n",
                "    filenames, labels,\n",
                "    test_size=1 - CONFIG['train_split'],\n",
                "    stratify=labels,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "train_file_to_label = {f: l for f, l in zip(train_files, train_labels)}\n",
                "val_file_to_label = {f: l for f, l in zip(val_files, val_labels)}\n",
                "\n",
                "# Verify\n",
                "overlap = set(train_files) & set(val_files)\n",
                "print(f\"\\nðŸ” Verification:\")\n",
                "print(f\"   Train: {len(train_files)}\")\n",
                "print(f\"   Val: {len(val_files)}\")\n",
                "print(f\"   Overlap: {len(overlap)} {'âœ…' if len(overlap) == 0 else 'âŒ'}\")\n",
                "\n",
                "if overlap:\n",
                "    raise ValueError(\"Data leakage!\")\n",
                "\n",
                "print(f\"\\nâœ… Split OK\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 6: Augmentation\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class Augmentation:\n",
                "    def __init__(self, config):\n",
                "        self.c = config\n",
                "    \n",
                "    def time_warp(self, x):\n",
                "        if np.random.rand() > self.c['aug_time_warp_prob']: return x\n",
                "        scale = np.random.uniform(*self.c['aug_time_warp_range'])\n",
                "        new_len = int(len(x) * scale)\n",
                "        new_len = np.clip(new_len, len(x)//2, len(x)*2)\n",
                "        idx = np.linspace(0, len(x)-1, new_len).astype(int)\n",
                "        return x[np.clip(idx, 0, len(x)-1)]\n",
                "    \n",
                "    def add_noise(self, x):\n",
                "        if np.random.rand() > self.c['aug_noise_prob']: return x\n",
                "        return x + np.random.normal(0, self.c['aug_noise_std'], x.shape).astype(np.float32)\n",
                "    \n",
                "    def rotate(self, x):\n",
                "        if np.random.rand() > self.c['aug_rotation_prob']: return x\n",
                "        angle = np.radians(np.random.uniform(*self.c['aug_rotation_range']))\n",
                "        R = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
                "        rotated = x.copy()\n",
                "        for i in range(0, x.shape[1], 3):\n",
                "            if i+1 < x.shape[1]:\n",
                "                rotated[:, i:i+2] = x[:, i:i+2] @ R.T\n",
                "        return rotated.astype(np.float32)\n",
                "    \n",
                "    def scale(self, x):\n",
                "        if np.random.rand() > self.c['aug_scaling_prob']: return x\n",
                "        s = np.random.uniform(*self.c['aug_scaling_range'])\n",
                "        return (x * s).astype(np.float32)\n",
                "    \n",
                "    def mask(self, x):\n",
                "        if np.random.rand() > self.c['aug_masking_prob']: return x\n",
                "        n = int(len(x) * self.c['aug_masking_ratio'])\n",
                "        if n > 0: x[np.random.choice(len(x), n, replace=False)] = 0\n",
                "        return x\n",
                "    \n",
                "    def shift(self, x):\n",
                "        if np.random.rand() > self.c['aug_temporal_shift_prob']: return x\n",
                "        s = np.random.randint(-len(x)//10, len(x)//10+1)\n",
                "        if s > 0: x = np.vstack([np.repeat(x[:1], s, 0), x[:-s]])\n",
                "        elif s < 0: x = np.vstack([x[-s:], np.repeat(x[-1:], -s, 0)])\n",
                "        return x\n",
                "    \n",
                "    def apply(self, x):\n",
                "        for fn in [self.time_warp, self.add_noise, self.rotate, self.scale, self.mask, self.shift]:\n",
                "            x = fn(x)\n",
                "        return x\n",
                "\n",
                "print(\"âœ… Augmentation ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 7: Dataset\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class ISLDataset(Dataset):\n",
                "    def __init__(self, cache_dir, file_to_label, label_to_id, seq_len=60, augment=False, config=None):\n",
                "        self.cache_dir = Path(cache_dir)\n",
                "        self.seq_len = seq_len\n",
                "        self.label_to_id = label_to_id\n",
                "        self.aug = Augmentation(config) if augment else None\n",
                "        self.samples = [(self.cache_dir/f, l) for f, l in file_to_label.items() if (self.cache_dir/f).exists()]\n",
                "        print(f\"   {len(self.samples)} samples, aug={augment}\")\n",
                "    \n",
                "    def __len__(self): \n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        path, label = self.samples[idx]\n",
                "        x = np.load(path).astype(np.float32)\n",
                "        \n",
                "        if self.aug: x = self.aug.apply(x)\n",
                "        \n",
                "        # Pad/trim\n",
                "        if len(x) >= self.seq_len:\n",
                "            idx = np.linspace(0, len(x)-1, self.seq_len, dtype=int)\n",
                "            x = x[idx]\n",
                "        else:\n",
                "            x = np.vstack([x, np.repeat(x[-1:], self.seq_len-len(x), 0)])\n",
                "        \n",
                "        # Normalize\n",
                "        x = (x - x.mean(0, keepdims=True)) / (x.std(0, keepdims=True) + 1e-8)\n",
                "        \n",
                "        return torch.from_numpy(x), self.label_to_id[label]\n",
                "\n",
                "# Create datasets\n",
                "print(\"\\nðŸ“¦ Creating datasets...\")\n",
                "train_ds = ISLDataset(CONFIG['cache_dir'], train_file_to_label, label_to_id, CONFIG['seq_len'], True, CONFIG)\n",
                "val_ds = ISLDataset(CONFIG['cache_dir'], val_file_to_label, label_to_id, CONFIG['seq_len'], False, CONFIG)\n",
                "\n",
                "train_loader = DataLoader(train_ds, CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
                "val_loader = DataLoader(val_ds, CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
                "\n",
                "print(f\"\\nâœ… Train: {len(train_ds)} samples, {len(train_loader)} batches\")\n",
                "print(f\"âœ… Val: {len(val_ds)} samples, {len(val_loader)} batches\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 8: Model\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class ISLModel(nn.Module):\n",
                "    def __init__(self, input_dim=408, hidden=512, classes=123, heads=8, layers=4, drop=0.3):\n",
                "        super().__init__()\n",
                "        self.embed = nn.Sequential(nn.Linear(input_dim, hidden), nn.LayerNorm(hidden), nn.GELU(), nn.Dropout(drop))\n",
                "        enc = nn.TransformerEncoderLayer(hidden, heads, hidden*4, drop, 'gelu', batch_first=True, norm_first=True)\n",
                "        self.transformer = nn.TransformerEncoder(enc, layers)\n",
                "        self.classifier = nn.Sequential(nn.Linear(hidden, hidden//2), nn.GELU(), nn.Dropout(drop), nn.Linear(hidden//2, classes))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.classifier(self.transformer(self.embed(x)).mean(1))\n",
                "\n",
                "model = ISLModel(\n",
                "    CONFIG['input_dim'], CONFIG['hidden_dim'], CONFIG['num_classes'],\n",
                "    CONFIG['num_heads'], CONFIG['num_layers'], CONFIG['dropout']\n",
                ").to(device)\n",
                "\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "print(\"âœ… Model ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 9: Training Functions\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
                "optimizer = torch.optim.AdamW(model.parameters(), CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
                "\n",
                "def train_epoch(model, loader):\n",
                "    model.train()\n",
                "    total_loss = correct = total = 0\n",
                "    for x, y in tqdm(loader, desc='Train'):\n",
                "        x, y = x.to(device), y.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        out = model(x)\n",
                "        loss = criterion(out, y)\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip'])\n",
                "        optimizer.step()\n",
                "        total_loss += loss.item()\n",
                "        correct += out.argmax(1).eq(y).sum().item()\n",
                "        total += len(y)\n",
                "    return total_loss/len(loader), 100*correct/total\n",
                "\n",
                "def validate(model, loader):\n",
                "    model.eval()\n",
                "    total_loss = correct = total = 0\n",
                "    with torch.no_grad():\n",
                "        for x, y in tqdm(loader, desc='Val'):\n",
                "            x, y = x.to(device), y.to(device)\n",
                "            out = model(x)\n",
                "            total_loss += criterion(out, y).item()\n",
                "            correct += out.argmax(1).eq(y).sum().item()\n",
                "            total += len(y)\n",
                "    return total_loss/len(loader), 100*correct/total\n",
                "\n",
                "print(\"âœ… Training functions ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 10: Main Training Loop\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ðŸš€ TRAINING\")\n",
                "print(\"=\"*60)\n",
                "print(\"ðŸŽ¯ Target: 73-78% val accuracy\\n\")\n",
                "\n",
                "best_acc = 0\n",
                "best_epoch = 0\n",
                "patience = 0\n",
                "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[], 'lr':[]}\n",
                "start = time.time()\n",
                "\n",
                "for epoch in range(CONFIG['epochs']):\n",
                "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
                "    \n",
                "    t_loss, t_acc = train_epoch(model, train_loader)\n",
                "    v_loss, v_acc = validate(model, val_loader)\n",
                "    scheduler.step()\n",
                "    \n",
                "    history['train_loss'].append(t_loss)\n",
                "    history['train_acc'].append(t_acc)\n",
                "    history['val_loss'].append(v_loss)\n",
                "    history['val_acc'].append(v_acc)\n",
                "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
                "    \n",
                "    print(f\"Train: {t_loss:.4f} / {t_acc:.2f}% | Val: {v_loss:.4f} / {v_acc:.2f}%\")\n",
                "    \n",
                "    if v_acc > best_acc + CONFIG['min_delta']:\n",
                "        best_acc = v_acc\n",
                "        best_epoch = epoch + 1\n",
                "        patience = 0\n",
                "        torch.save({\n",
                "            'epoch': epoch+1, 'model': model.state_dict(), 'acc': best_acc,\n",
                "            'config': CONFIG, 'history': history\n",
                "        }, f\"{CONFIG['save_dir']}/best_isl_123.pth\")\n",
                "        print(f\"âœ… NEW BEST: {v_acc:.2f}%\")\n",
                "        if v_acc >= 75: print(\"ðŸŽ¯ TARGET!\")\n",
                "    else:\n",
                "        patience += 1\n",
                "        print(f\"({patience}/{CONFIG['patience']})\")\n",
                "    \n",
                "    if patience >= CONFIG['patience']:\n",
                "        print(f\"\\nEarly stop at epoch {epoch+1}\")\n",
                "        break\n",
                "\n",
                "elapsed = time.time() - start\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Best: {best_acc:.2f}% (epoch {best_epoch})\")\n",
                "print(f\"Time: {elapsed/60:.2f} min\")\n",
                "\n",
                "with open(f\"{CONFIG['save_dir']}/history.json\", 'w') as f:\n",
                "    json.dump(history, f, indent=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 11: Plot Results\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "ax1.plot(history['train_loss'], 'o-', label='Train', alpha=0.8)\n",
                "ax1.plot(history['val_loss'], 's-', label='Val', alpha=0.8)\n",
                "ax1.axvline(best_epoch-1, color='r', linestyle='--', alpha=0.5)\n",
                "ax1.set(xlabel='Epoch', ylabel='Loss', title='Loss')\n",
                "ax1.legend(); ax1.grid(alpha=0.3)\n",
                "\n",
                "ax2.plot(history['train_acc'], 'o-', label='Train', alpha=0.8)\n",
                "ax2.plot(history['val_acc'], 's-', label='Val', alpha=0.8)\n",
                "ax2.axvline(best_epoch-1, color='r', linestyle='--', alpha=0.5)\n",
                "ax2.axhline(best_acc, color='g', linestyle=':', alpha=0.5)\n",
                "ax2.axhline(75, color='purple', linestyle='--', alpha=0.5, label='Target')\n",
                "ax2.set(xlabel='Epoch', ylabel='Accuracy (%)', title=f'Accuracy (Best: {best_acc:.2f}%)')\n",
                "ax2.legend(); ax2.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{CONFIG['save_dir']}/results.png\", dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… Plotted\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 12: Summary\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "print(\"=\"*60)\n",
                "print(\"ðŸ“‹ SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nâœ… Best Val Acc: {best_acc:.2f}% (Epoch {best_epoch})\")\n",
                "print(f\"âœ… Time: {elapsed/60:.2f} minutes\")\n",
                "print(f\"âœ… Classes: {len(class_counts)}\")\n",
                "print(f\"âœ… Samples: {len(file_to_label)}\")\n",
                "\n",
                "print(f\"\\nðŸ’¾ Saved:\")\n",
                "print(f\"   - file_to_label.json\")\n",
                "print(f\"   - best_isl_123.pth\")\n",
                "print(f\"   - history.json\")\n",
                "print(f\"   - results.png\")\n",
                "\n",
                "if best_acc >= 75:\n",
                "    print(f\"\\nðŸŽ‰ SUCCESS! {best_acc:.2f}% >= 75%\")\n",
                "elif best_acc >= 73:\n",
                "    print(f\"\\nðŸ‘ VERY CLOSE! {best_acc:.2f}%\")\n",
                "elif best_acc > 70:\n",
                "    print(f\"\\nðŸ“ˆ IMPROVED! {best_acc:.2f}% (baseline 70%)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… DONE\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
SignSpeak: A Comprehensive Technical Analysis and Implementation Strategy for Bidirectional Real-Time Sign Language Translation
1. Executive Summary and Sociotechnical Context
The technological imperative to bridge the communication chasm between the Deaf and Hard-of-Hearing (DHH) community and the hearing majority has never been more pressing. With approximately 1.3 billion people globally living with some form of disability, and a significant portion of this demographic relying on sign language as their primary mode of communication, the development of robust, bidirectional translation systems is not merely an academic exercise but a fundamental requirement for digital equity.1 The project titled "SignSpeak: A Bidirectional Real-Time Sign Language Translation System," proposed for a final year engineering capstone at Anna University, situates itself at the intersection of computer vision, natural language processing (NLP), and human-computer interaction (HCI). This report provides an exhaustive, expert-level analysis of the current state-of-the-art, derived from a rigorous synthesis of foundational literature and existing implementations, to chart a viable roadmap for developing a high-fidelity Indian Sign Language (ISL) translator.
The complexity of this undertaking cannot be overstated. Unlike spoken languages, which are linear and acoustic, sign languages are multilinear and visual-spatial. They employ a concurrent stream of manual articulators—hand shape, palm orientation, movement, and location—alongside critical non-manual markers (NMMs) such as facial expressions, head tilts, and body posture. Indian Sign Language (ISL), in particular, possesses its own distinct grammar, syntax (typically Subject-Object-Verb), and lexicon, independent of spoken languages like English, Tamil, or Hindi.2 A truly bidirectional system must therefore master two distinct translation tasks: Sign Language Recognition (SLR), which decodes this visual stream into text or speech, and Sign Language Production (SLP), which encodes text or speech into accurate, linguistically valid 3D avatar animations.
This report dissects three primary base papers provided for analysis—covering multimodal adaptive learning, self-supervised video foundation models, and speech-to-action frameworks—and integrates them with a broader review of existing "SignSpeak" implementations found in current research. The analysis reveals that while significant strides have been made in isolated gesture recognition, the seamless, real-time, bidirectional translation of continuous ISL remains an open research challenge. By leveraging hybrid CNN-Transformer architectures, foundational video masked autoencoders, and context-aware NLP pipelines, this project has the potential to advance the field significantly.
2. Deep Analysis of Foundational Literature
To establish a robust theoretical framework for "SignSpeak," we must first deconstruct the architectural principles presented in the primary source materials. These papers offer distinct but complementary methodologies for handling the multimodal nature of sign language communication.
2.1 Hybrid CNN-Transformer Architectures for Real-Time Recognition
The paper A Multimodal AI-Driven Framework for Adaptive Learning of Differently-Abled Learners 1 provides a critical architectural blueprint for the recognition component of SignSpeak. The authors propose a system that achieves an impressive 96.8% accuracy in recognizing American Sign Language (ASL) in educational settings, a metric that sets a high benchmark for the proposed ISL system.
2.1.1 Architectural Deconstruction
The core innovation lies in the Hybrid CNN-Transformer architecture. Traditional approaches have often relied on Convolutional Neural Networks (CNNs) for spatial feature extraction or Recurrent Neural Networks (RNNs/LSTMs) for temporal sequencing. However, RNNs often suffer from vanishing gradient problems over long sequences, and pure CNNs struggle to capture global temporal dependencies.
The hybrid model addresses this by utilizing a CNN as a sophisticated feature extractor. As detailed in the study, the system processes video input—specifically 32-frame sequences—extracting 147-dimensional feature vectors per frame.1 This extraction is not merely pixel-based; it integrates MediaPipe hand tracking to derive high-level skeletal data (21 3D landmarks per hand), which is then augmented with derived features such as hand velocity vectors, palm orientation angles, and inter-finger angles.
The mathematical formulation for this feature extraction is rigorous:


Here,  represents the landmark coordinates at time ,  captures the velocity (critical for distinguishing dynamic signs),  represents the geometric inter-finger angles, and  is the concatenated feature vector.1
This feature vector  is then passed into a Transformer encoder. Unlike LSTMs, which process data sequentially, the Transformer employs Multi-Head Self-Attention, allowing the model to weigh the relevance of every frame in the sequence against every other frame simultaneously. This is crucial for handling co-articulation in sign language—the phenomenon where the formation of a sign is influenced by the signs preceding and following it. The attention mechanism is defined as:

Where , , and  are the Query, Key, and Value matrices derived from the CNN features. This architecture enables the system to capture long-range dependencies—essential for understanding context in continuous signing—while maintaining real-time performance with inference latencies below 300ms.1
2.1.2 Implications for SignSpeak
For the Anna University project, this validates the shift away from purely pixel-based approaches. Implementing a feature-extraction pipeline using MediaPipe (or similar pose estimators) fed into a Transformer backbone is the optimal strategy for balancing accuracy with the computational constraints of real-time deployment. The reported 96.8% accuracy suggests that this architecture is robust enough for educational and communicative applications.
2.2 Foundation Models and Self-Supervised Learning
The second paper, From Data to Signs: A Foundation Model for Multilingual Sign Language Recognition 1, addresses the most significant bottleneck in ISL research: data scarcity. Deep learning models are notoriously data-hungry, and high-quality, annotated ISL datasets are rare compared to their spoken language counterparts.
2.2.1 The VideoMAE Paradigm
The researchers introduce a Video Masked Autoencoder (VideoMAE) built upon a Vision Transformer (ViT-L) backbone. This approach represents a paradigm shift from supervised learning (which requires labelled data) to Self-Supervised Learning (SSL).
The core mechanism is Tube Masking. The model treats a video as a 3D volume of data and partitions it into spatio-temporal "tubes." During pre-training, an extremely high percentage of these tubes—90%—are masked (hidden) from the model. The network's objective is to reconstruct the missing pixels based on the remaining 10% of visible information.

By forcing the model to reconstruct the video from such sparse data, it is compelled to learn high-level, semantic representations of motion and structure rather than relying on static background cues or surface textures.1
2.2.2 Transfer Learning and Generalization
The study demonstrates that pre-training this model on large, general-purpose datasets like Kinetics-400 or large-scale sign language datasets like YouTube-ASL (even without precise gloss annotations) allows the model to learn the fundamental "physics" of human gesture.1 When this pre-trained foundation model is subsequently fine-tuned on specific languages (such as Russian, American, Greek, or Turkish Sign Language), it converges up to 3.5 times faster and achieves state-of-the-art results (e.g., SOTA on the Slovo dataset) compared to training from scratch.1
2.2.3 Relevance to Indian Sign Language
This methodology is a game-changer for the SignSpeak project. Instead of spending months collecting thousands of ISL samples, the project can utilize pre-trained weights from a VideoMAE model trained on YouTube-ASL. By identifying the "transferable" features of sign language—hand shapes and motion primitives common across languages—the team can achieve high recognition accuracy on ISL with a fraction of the training data. This effectively solves the "cold start" problem for under-resourced languages like ISL.
2.3 Speech-to-Action and Intent Recognition
The third paper, Structured and Unstructured Speech2Action Frameworks for Human-Robot Collaboration 1, provides the theoretical underpinning for the "Speech-to-Sign" (Reverse Translation) component of the project. While the paper focuses on robotics, the mapping of spoken language to executable commands is structurally identical to mapping speech to sign language glosses.
2.3.1 Structured vs. Unstructured Interactions
The study differentiates between Structured Speech (rigid, scripted commands) and Unstructured Speech (natural, conversational language). It utilizes a pipeline comprising Google Cloud Speech-to-Text for transcription, BERT for intent recognition, and GPT-Neo for command generation.1
For SignSpeak, this distinction is vital. A basic system might only map spoken words directly to signs (Structured), leading to grammatically incorrect ISL (e.g., "I am going home" -> "I" + "AM" + "GOING" + "HOME"). However, a robust system must handle Unstructured natural language, interpreting the user's meaning and restructuring it into valid ISL syntax (e.g., "HOME I GO"). The integration of Large Language Models (LLMs) like GPT-Neo or BERT allows the system to perform this Text-to-Gloss translation, converting SVO English sentences into the SOV (Subject-Object-Verb) structure typical of ISL, while also filtering out non-lexical filler words.
2.3.2 The Pratfall Effect and Error Tolerance
An intriguing insight from this study is the Pratfall Effect, where minor errors in execution (by a robot or system) do not necessarily degrade usability if the system is perceived as competent and safe.1 In the context of SignSpeak, this suggests that while 100% translation accuracy is ideal, a system that captures the core intent of the message, even if it misses a minor grammatical nuance in the sign animation, remains highly valuable to the user. This validates an iterative development approach where "good enough" communication is the immediate goal, followed by linguistic perfection.
3. The "SignSpeak" Landscape: Analysis of Existing Implementations
To understand "how this project is implemented yet," we must look beyond the theoretical papers to the actual implementations of "SignSpeak" and similar systems documented in the provided research snippets. Several projects sharing the name "SignSpeak" or exploring identical territory have been identified, revealing a diverse landscape of approaches and varying degrees of success.
3.1 Review of Existing "SignSpeak" Projects
The research snippets reveal multiple distinct initiatives under the "SignSpeak" moniker, highlighting that this is a crowded and active research field.
    1. SignSpeak (Mobile/Android Focus): One implementation 3 describes an Android application designed to facilitate two-way communication. It utilizes TensorFlow Lite for efficient on-device gesture recognition and Firebase for backend support. This system emphasizes accessibility, employing Convolutional Neural Networks (CNNs) for classification. However, its reliance on standard CNNs likely limits its ability to handle continuous sentences effectively compared to the hybrid architectures discussed in Section 2.1.
    2. SignSpeak (Web/Browser Focus): Another implementation 4 presents a privacy-friendly web application running entirely in the browser. It leverages MediaPipe Hand Landmarker and TensorFlow.js with a K-Nearest Neighbors (KNN) classifier. While this ensures low latency and privacy (no data sent to servers), KNN is a simplistic classifier that cannot model the temporal dynamics of complex signs, limiting it to static alphabet or number recognition.
    3. SignSpeak (Deep Learning/LSTM Focus): A more robust implementation 5 combines BERT for text processing and OpenAI's Whisper model for Automatic Speech Recognition (ASR). This system translates audio input into text, reorders it using NLP (likely BERT), and then retrieves and stitches together sign gloss videos (using FFmpeg) to generate the output. This "concatenative synthesis" approach is effective for basic communication but lacks the fluidity and prosody of avatar-based systems.
    4. SignSpeak (Glove-Based Approach): Snippet 6 details a hardware-based "SignSpeak" project using a data glove with flex sensors and an Arduino Mega. While it achieves 92% accuracy, glove-based systems are generally considered intrusive, expensive, and less scalable than vision-based systems, hindering widespread adoption.
3.2 State of the Art in Indian Sign Language (ISL) Recognition
Focusing specifically on the Indian context, the current state of implementation involves several key technologies and datasets.
3.2.1 Data Ecosystem
The "data bottleneck" is slowly being alleviated by new corpora.
    • INCLUDE Dataset: This is a primary resource, containing 4,287 videos across 263 word classes, recorded by experienced signers.7 It is a standard benchmark for ISL recognition tasks.
    • CISLR Corpus: A more recent introduction, the Corpus for Indian Sign Language Recognition (CISLR), aims to provide a benchmark for word-level recognition, addressing the gap in continuous ISL resources.8
    • ISL-CSLTR: This dataset specifically targets continuous sign language translation and recognition, offering sentence-level annotations crucial for training models to understand context and co-articulation.10
3.2.2 Recognition Methodologies
Current ISL recognition systems are moving toward MediaPipe Holistic as the standard for input processing. By tracking face, hand, and pose landmarks simultaneously, these systems capture the full range of ISL articulators.11
    • LSTM & GRU Models: The dominant architecture for temporal modeling in ISL remains the Long Short-Term Memory (LSTM) network. Studies report accuracies of 88% to 94% on custom ISL datasets using MediaPipe features fed into LSTMs.13
    • Continuous Recognition (SignFlow): More advanced research 16 introduces SignFlow, a network designed for real-time continuous ISL recognition. It utilizes a pre-trained CNN for spatial features and a Transformer for temporal dynamics, optimized with Connectionist Temporal Classification (CTC) loss. This allows it to decode sequences of signs without explicit segmentation, achieving a Word Error Rate (WER) of 19% on continuous ISL data.
3.3 State of the Art in ISL Production (Avatar Generation)
The "reverse" translation—from text/audio to ISL—relies heavily on avatar technology.
    • HamNoSys and SiGML: The standard pipeline for ISL production involves converting text to HamNoSys (Hamburg Notation System) and then to SiGML (Signing Gesture Markup Language). Tools like the JA SiGML URL App or eSIGNEditor are used to render these XML descriptions into 3D animations.17
    • Limitations: While effective, this pipeline is rule-based and rigid. Constructing the dictionary mapping English words to HamNoSys strings for ISL is a manual, labor-intensive process. Furthermore, the resulting animations often lack the "naturalness" and emotional expressivity of a human signer, leading to the "robotic" quality criticized in user studies.18
    • 3D Avatar Toolkits: Projects like Sign-Kit 19 provide web-based toolkits for converting speech to ISL gestures using 3D avatars. These systems often use the MERN stack and WebGL for rendering, achieving high usability scores but struggling with the limited vocabulary of pre-defined SiGML sequences.
4. Advanced Implementation Roadmap: How to Build It
Drawing from the rigorous analysis above, this section outlines a comprehensive, technically superior implementation roadmap for the "SignSpeak" project at Anna University. This roadmap moves beyond the basic prototypes discussed in Section 3 to incorporate the advanced architectures from Section 2.
4.1 System Architecture Overview
The proposed system is a modular, bidirectional pipeline designed for scalability and real-time performance.
    • Frontend (User Interface): A cross-platform mobile application (Flutter/React Native) serving as the interaction point. It captures video for recognition and renders the 3D avatar for production.
    • Edge Inference Layer: Lightweight models (TFLite) running on-device for initial hand detection and tracking (MediaPipe) to minimize latency.
    • Cloud Processing Layer: Heavy-duty Transformer models and NLP engines hosted on a scalable backend (AWS/GCP via Docker containers) for complex translation and semantic processing.
    • Federated Learning Module: A privacy-preserving training loop that updates the global model based on local user corrections without transmitting raw video data.
4.2 Module A: Real-Time ISL Recognition (The "Eyes")
Objective: Convert continuous ISL gestures into English/Tamil text.
Implementation Strategy:
    1. Input Preprocessing (MediaPipe Holistic):
        ◦ Capture video frames at 30 FPS.
        ◦ Apply MediaPipe Holistic to extract 543 landmarks (33 Pose, 468 Face, 21x2 Hands) per frame.
        ◦ Normalization: Crucial step. Normalize all coordinates relative to the user's skeletal center (e.g., the midpoint of the shoulders) to ensure the model is invariant to the user's distance from the camera.
        ◦ Dimensionality Reduction: Drop non-essential facial landmarks (keep only eyes, eyebrows, and mouth contours) to reduce the input vector size and improve processing speed.
    2. Feature Engineering:
        ◦ Calculate relative features: Distances between fingertips and palm center (for hand shape), angles between bone vectors (for finger curl), and velocity of the wrist (for movement dynamics).
        ◦ This enriches the raw coordinate data, making it easier for the model to learn distinct signs.
    3. Model Architecture (Hybrid Transformer):
        ◦ Adopt the Hybrid CNN-Transformer approach 1 or a Graph Convolutional Network (GCN) combined with a Transformer.
        ◦ Spatial Encoder: Use a lightweight Graph Neural Network (GCN) to process the skeletal graph of each frame, encoding the spatial relationships between joints.
        ◦ Temporal Encoder: Feed the sequence of spatial embeddings into a Transformer Encoder (4-6 layers, 8 heads). Add Positional Encodings to retain sequence order.
        ◦ Classification Head: A Fully Connected (Dense) layer with Softmax activation to predict probabilities over the ISL vocabulary (classes).
    4. Continuous Decoding (CTC):
        ◦ Use Connectionist Temporal Classification (CTC) loss during training. This allows the model to output a sequence of glosses (e.g., "HELLO", "HOW", "YOU") from a continuous stream of unsegmented video, automatically handling the variable duration of each sign and the null transitions between them.16
4.3 Module B: Speech-to-ISL Translation (The "Voice")
Objective: Convert spoken English/Tamil into fluid, grammatically correct ISL animations.
Implementation Strategy:
    1. Automatic Speech Recognition (ASR):
        ◦ Integrate OpenAI Whisper (Base or Small model) via API or local inference. Whisper is chosen for its superior robustness to Indian accents and background noise compared to traditional engines.5
    2. Neural Translation (NLP Engine):
        ◦ Text-to-Gloss: Do not simply map words to signs. Train a Sequence-to-Sequence (Seq2Seq) model (like T5 or BART) to translate English sentences into ISL Gloss sequences.
        ◦ Training Data: Use the ISL-CSLTR dataset or create a synthetic dataset mapping English grammar to ISL grammar (SOV, removal of copula verbs, wh-questions at the end).
        ◦ Example: Input "Where are you going?"  Model  Output "YOU GO WHERE?"
    3. Avatar Animation (The Production Engine):
        ◦ Platform: Use Unity 3D for the visualization engine.
        ◦ Animation System: Develop a blending system. Instead of playing discrete clips (which looks jerky), use Unity's Mecanim system or cross-fading to smooth the transitions between sign animations.
        ◦ Non-Manual Markers: This is a critical improvement. Control the avatar's facial morph targets (blendshapes) based on the NLP output. If the sentence is a question, the avatar's eyebrows should furrow or raise automatically.
4.4 The Fusion Mechanism
As highlighted in the Multimodal Framework paper 1, a robust system should handle conflicting inputs.
    • Conflict Resolution: If the camera detects a sign for "YES" but the microphone detects speech saying "NO", the system uses a Confidence-Weighted Fusion algorithm (Algorithm 5 in 1). It calculates the entropy (uncertainty) of each modality and prioritizes the one with higher confidence, ensuring reliable output even in noisy or visually cluttered environments.
5. Improvements Further from Previous Implementations
The user specifically asks "what can we improve further." Based on the gaps identified in current literature and snippets, the following advanced features are proposed to elevate the project to a research-grade contribution.
5.1 Innovation 1: Privacy-Preserving Federated Learning
Current Limitation: Most apps require uploading video data to the cloud for processing, which is a major privacy concern for users, especially in private settings. Proposed Improvement: Implement Federated Learning.1 The model training happens locally on the user's device. When the user corrects a translation error, the model learns from this interaction. Only the mathematical update (gradients) is sent to the central server to improve the global model—the video data never leaves the phone. This addresses privacy compliance and allows the model to collectively learn from thousands of users without centralizing sensitive data.
5.2 Innovation 2: Emotion-Aware Avatar Generation
Current Limitation: Avatars are often described as "robotic" or "zombie-like" because they lack appropriate facial affect, which is grammatically mandatory in sign language.18 Proposed Improvement: Integrate an Emotion Recognition module in the NLP pipeline. Using sentiment analysis (e.g., VADER or BERT-based sentiment), determine the emotional tone of the input speech (Happy, Sad, Angry, Inquisitive). Map these tones to specific Facial Blendshapes on the Unity avatar.
    • Implementation: If Sentiment = "Angry", set Avatar.SetBlendShapeWeight("EyebrowsDown", 100). This makes the translation not just linguistically accurate but prosodically correct.
5.3 Innovation 3: Handling Regional Dialects
Current Limitation: ISL varies significantly across regions (e.g., Mumbai vs. Delhi vs. Kolkata variations). Most models are trained on a "standard" dialect that may not work for all users.20 Proposed Improvement: Introduce a User Profile/Calibration step. Upon first launch, ask the user to sign 5-10 key words that vary by region. Use Few-Shot Learning (specifically the Prototypical Networks approach mentioned in 9) to fine-tune the classification layer to the user's specific dialect. This personalization significantly boosts usability.
5.4 Innovation 4: Bidirectional "Fingerspelling" Fallback
Current Limitation: No SLR system has a complete vocabulary. When a user signs a proper noun (like a name) that isn't in the database, the system fails.
Proposed Improvement: Implement a robust Fingerspelling Recognition module as a fallback. If the confidence for a word-level sign is low, the system should automatically switch to a character-level recognition mode to decode fingerspelled words. Conversely, in the Speech-to-Sign direction, if a word (e.g., "ChatGPT") has no dictionary sign, the avatar should automatically fingerspell it letter-by-letter.
5.5 Innovation 5: Edge Optimization with Quantization
Current Limitation: Hybrid Transformers are computationally heavy for mobile phones.
Proposed Improvement: Apply Post-Training Quantization. Convert the model weights from 32-bit floating point (FP32) to 8-bit integers (INT8). This reduces the model size by 4x and increases inference speed on mobile CPUs/NPUs with negligible loss in accuracy. This is critical for deployment in India, where mid-range Android devices are prevalent.
6. Conclusion
The "SignSpeak" project represents a vital convergence of advanced AI research and social utility. By moving beyond simple dictionary-lookup methods and embracing the sophisticated architectures of Hybrid CNN-Transformers and VideoMAE Foundation Models, the project can achieve the robustness required for real-world use. The integration of Federated Learning for privacy, Emotion AI for natural interaction, and MediaPipe-based optimizations for real-time edge performance addresses the key limitations of existing systems.
For the students of Anna University, the path forward involves a disciplined execution of the roadmap: establishing the MediaPipe-Transformer pipeline, curating a clean dataset from sources like INCLUDE and CISLR, and focusing heavily on the "fusion" of modalities. Success in this project will not only yield a technically impressive engineering capstone but will contribute a tangible, scalable solution to the global challenge of accessible communication for the Deaf and Hard-of-Hearing community.
7. References (Integrated)
    • 1
Kodur, K., et al. "Structured and Unstructured Speech2Action Frameworks for Human-Robot Collaboration: A User Study." IEEE Access, 2026.
    • 1
Novopoltsev, M. Y., et al. "From Data to Signs: A Foundation Model for Multilingual Sign Language Recognition." IEEE Access, 2025.
    • 1
Saldanha, A. D., et al. "A Multimodal AI-Driven Framework for Adaptive Learning of Differently-Abled Learners Using Deep Learning and Real-Time Gesture Recognition." IEEE Access, 2026.
    • 11
"A Continuous SLR System using LSTM and MediaPipe." Wireless Personal Communications, 2024.
    • 3
"SignSpeak: Android Application for Two-Way Communication." IRJAEH.
    • 5
"SignSpeak: Real-time Translation System using BERT and Whisper." International Journal of Scientific Research and Engineering Development.
    • 16
Geetha, M., et al. "SignFlow: Real-Time Recognition of Continuous Indian Sign Language." IEEE Access.
    • 8
Joshi, A., et al. "CISLR: Corpus for Indian Sign Language Recognition." EMNLP 2022.
    • 2
"Harnessing AI to Generate Indian Sign Language from Natural Speech." The SAI Organization.
    • 7
Sridhar, et al. "INCLUDE: Indian Lexicon Sign Language Dataset." European Language Grid.
(End of Report)
Works cited
    1. A_Multimodal_AI-Driven_Framework_for_Adaptive_Learning_of_Differently-Abled_Learners_Using_Deep_Learning_and_Real-Time_Gesture_Recognition.pdf
    2. Harnessing AI to Generate Indian Sign Language from Natural Speech and Text for Digital Inclusion and Accessibility - The Science and Information (SAI) Organization, accessed January 24, 2026, https://thesai.org/Downloads/Volume15No4/Paper_114-Harnessing_AI_to_Generate_Indian_Sign_Language_from_Natural_Speech.pdf
    3. SignSpeak: Sign Language-To-Speech and Speech-To-Sign Language - International Research Journal on Advanced Engineering Hub (IRJAEH), accessed January 24, 2026, https://irjaeh.com/index.php/journal/article/download/569/512/1132
    4. divyesh1099/signSpeak: Create your own Sign language and then translate it live. - GitHub, accessed January 24, 2026, https://github.com/divyesh1099/signSpeak
    5. SignSpeak – Sign language translation system for hearing impaired - | International Journal of Science and Research Archive, accessed January 24, 2026, https://journalijsra.com/sites/default/files/fulltext_pdf/IJSRA-2025-1523.pdf
    6. SignSpeak: Open-Source Time Series Classification for ASL Translation - GitHub, accessed January 24, 2026, https://github.com/adityamakkar000/ASL-Sign-Research
    7. ELG - INCLUDE: A Large Scale Dataset for Indian Sign Language Recognition, accessed January 24, 2026, https://live.european-language-grid.eu/catalogue/lcr/7631
    8. iSign: A Benchmark for Indian Sign Language Processing - Exploration-Lab, accessed January 24, 2026, https://exploration-lab.github.io/iSign/
    9. CISLR: Corpus for Indian Sign Language Recognition - ACL Anthology, accessed January 24, 2026, https://aclanthology.org/2022.emnlp-main.707.pdf
    10. ISL-CSLTR: Indian Sign Language Dataset for Continuous Sign Language Translation and Recognition - Mendeley Data, accessed January 24, 2026, https://data.mendeley.com/datasets/kcmpdxky7p/1
    11. [2411.04517] Continuous Sign Language Recognition System using Deep Learning with MediaPipe Holistic - arXiv, accessed January 24, 2026, https://arxiv.org/abs/2411.04517
    12. Continuous Sign Language Recognition Using LSTM and Media Pipe Holistic, accessed January 24, 2026, https://ijsred.com/volume6/issue5/IJSRED-V6I5P20.pdf
    13. Real-Time Indian Sign Language Recognition & Multilingual Sign Generation - IEEE Xplore, accessed January 24, 2026, https://ieeexplore.ieee.org/iel8/11040159/11041696/11041851.pdf
    14. MP-GestLSTM: real time gesture detection using MediaPipe and LSTM - Taylor & Francis, accessed January 24, 2026, https://www.tandfonline.com/doi/full/10.1080/21642583.2025.2587853
    15. Continuous Sign Language Recognition System Using Deep Learning with MediaPipe Holistic | Request PDF - ResearchGate, accessed January 24, 2026, https://www.researchgate.net/publication/382301146_Continuous_Sign_Language_Recognition_System_Using_Deep_Learning_with_MediaPipe_Holistic
    16. Towards Real-Time Recognition of Continuous Indian Sign Language: A Multi-Modal Approach using RGB and Pose - IEEE Xplore, accessed January 24, 2026, https://ieeexplore.ieee.org/iel8/6287639/6514899/10938618.pdf
    17. HamNoSys to SiGML Conversion System for Sign Language Automation - ResearchGate, accessed January 24, 2026, https://www.researchgate.net/publication/306362600_HamNoSys_to_SiGML_Conversion_System_for_Sign_Language_Automation
    18. Manual Evaluation of synthesised Sign Language Avatars - Arrow@TU Dublin, accessed January 24, 2026, https://arrow.tudublin.ie/context/itbinfocon/article/1013/viewcontent/Manual_Evaluation_of_Synthesised_Sign_Language_Avatars.pdf
    19. spectre900/Sign-Kit-An-Avatar-based-ISL-Toolkit - GitHub, accessed January 24, 2026, https://github.com/spectre900/Sign-Kit-An-Avatar-based-ISL-Toolkit
    20. Indian Sign Language Recognition (ISLAR) - YouTube, accessed January 24, 2026, https://www.youtube.com/watch?v=s_1Uh2fE9pI
